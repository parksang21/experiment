{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append('/workspace/experiment')\n",
    "from data.cifar10 import SplitCifar10, train_transform, val_transform\n",
    "from data.capsule_split import get_splits\n",
    "\n",
    "import os\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "weight_path = '/workspace/experiment/log/NoiseInjection/cifar10_s0/0'\n",
    "os.path.exists(weight_path)\n",
    "\n",
    "checkpoint = torch.load(weight_path+'/epoch=99-step=11799.ckpt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class NoiseLayer(nn.Module):\n",
    "    def __init__(self, alpha, num_classes):\n",
    "        super(NoiseLayer, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.num_classes = torch.arange(num_classes)\n",
    "        \n",
    "    def calculate_class_mean(self, \n",
    "                           x: torch.Tensor, \n",
    "                           y: torch.Tensor):\n",
    "        \"\"\"calculate the variance of each classes' noise\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): [input tensor]\n",
    "            y (torch.Tensor): [target tensor]\n",
    "\n",
    "        Returns:\n",
    "            [Tensor]: [returns class dependent noise variance]\n",
    "        \"\"\"\n",
    "        self.num_classes = self.num_classes.type_as(y)\n",
    "        idxs = y.unsqueeze(0) == self.num_classes.unsqueeze(1)\n",
    "        mean = []\n",
    "        std = []\n",
    "        for i in range(self.num_classes.shape[0]):\n",
    "            x_ = x[idxs[i]]\n",
    "            mean.append(x_.mean(0))\n",
    "            std.append(x_.std(0))\n",
    "        \n",
    "        return torch.stack(mean), torch.stack(std)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.size(0)\n",
    "        class_mean, class_var = self.calculate_class_mean(x, y)\n",
    "        \n",
    "        # class_noise = torch.normal(mean=class_mean, std=class_var).type_as(x).detach()\n",
    "        class_noise = torch.normal(mean=0., std=class_var).type_as(x).detach()\n",
    "\n",
    "        index = torch.randperm(batch_size).type_as(y)\n",
    "        newY = y[index]\n",
    "        mask = y != newY\n",
    "        if x.dim() == 2:\n",
    "            mask = mask.unsqueeze(1).expand_as(x).type_as(x)\n",
    "        else:\n",
    "            mask = mask[...,None,None,None].expand_as(x).type_as(x)\n",
    "        \n",
    "        return x + self.alpha * class_noise[newY], newY\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.05)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class classifier32(nn.Module):\n",
    "    def __init__(self, num_classes=2, alpha=0.5, **kwargs):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = nn.Conv2d(3,       64,     3, 1, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64,      64,     3, 1, 1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64,     128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv5 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv6 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv8 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv9 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        self.bn9 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, num_classes)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "        self.dr2 = nn.Dropout2d(0.2)\n",
    "        self.dr3 = nn.Dropout2d(0.2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.noiseLayer = NoiseLayer(alpha, num_classes)\n",
    "\n",
    "    def forward(self, x, y, return_features=[], noise=[]):\n",
    "        batch_size = len(x)\n",
    "        out_feat = []\n",
    "\n",
    "        x = self.dr1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        l1 = nn.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        if 0 in return_features:\n",
    "            out_feat.append(l1)\n",
    "            \n",
    "        if 0 in noise:\n",
    "            l1, ny = self.noiseLayer(l1, y)\n",
    "\n",
    "        x = self.dr2(l1)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        l2 = nn.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        if 1 in return_features:\n",
    "            out_feat.append(l2)\n",
    "            \n",
    "        if 1 in noise:\n",
    "            l2, ny = self.noiseLayer(l2, y)\n",
    "\n",
    "        x = self.dr3(l2)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.bn9(x)\n",
    "        l3 = nn.LeakyReLU(0.2)(x)\n",
    "        \n",
    "        l3 = self.avgpool(l3)\n",
    "        l3 = l3.view(batch_size, -1)\n",
    "        \n",
    "        if 2 in return_features:\n",
    "            out_feat.append(l3)\n",
    "        \n",
    "        if 2 in noise:\n",
    "            l3, ny = self.noiseLayer(l3, y)\n",
    "        \n",
    "        y = self.fc1(l3)\n",
    "        \n",
    "        if len(return_features) > 0:\n",
    "            if len(return_features) == 1:\n",
    "                out_feat = out_feat[0]\n",
    "\n",
    "        if len(noise) > 0:\n",
    "            return y, out_feat, ny\n",
    "        \n",
    "        return y, out_feat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "checkpoint.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'hparams_name', 'hyper_parameters'])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "checkpoint['state_dict']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "odict_keys(['model.conv1.weight', 'model.conv2.weight', 'model.conv3.weight', 'model.conv4.weight', 'model.conv5.weight', 'model.conv6.weight', 'model.conv7.weight', 'model.conv8.weight', 'model.conv9.weight', 'model.bn1.weight', 'model.bn1.bias', 'model.bn1.running_mean', 'model.bn1.running_var', 'model.bn1.num_batches_tracked', 'model.bn2.weight', 'model.bn2.bias', 'model.bn2.running_mean', 'model.bn2.running_var', 'model.bn2.num_batches_tracked', 'model.bn3.weight', 'model.bn3.bias', 'model.bn3.running_mean', 'model.bn3.running_var', 'model.bn3.num_batches_tracked', 'model.bn4.weight', 'model.bn4.bias', 'model.bn4.running_mean', 'model.bn4.running_var', 'model.bn4.num_batches_tracked', 'model.bn5.weight', 'model.bn5.bias', 'model.bn5.running_mean', 'model.bn5.running_var', 'model.bn5.num_batches_tracked', 'model.bn6.weight', 'model.bn6.bias', 'model.bn6.running_mean', 'model.bn6.running_var', 'model.bn6.num_batches_tracked', 'model.bn7.weight', 'model.bn7.bias', 'model.bn7.running_mean', 'model.bn7.running_var', 'model.bn7.num_batches_tracked', 'model.bn8.weight', 'model.bn8.bias', 'model.bn8.running_mean', 'model.bn8.running_var', 'model.bn8.num_batches_tracked', 'model.bn9.weight', 'model.bn9.bias', 'model.bn9.running_mean', 'model.bn9.running_var', 'model.bn9.num_batches_tracked', 'model.fc1.weight', 'model.fc1.bias', 'dummyFC.weight', 'dummyFC.bias'])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model = classifier32(num_classes=6, alpha=0.5)\n",
    "model_state_dict = model.state_dict()\n",
    "for name, param in checkpoint['state_dict'].items():\n",
    "    n = name.replace('model.', '')\n",
    "    if n in model_state_dict.keys():\n",
    "        model_state_dict[n].copy_(param)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "dummyFC = nn.Linear(128, 6)\n",
    "dummyFC_SD = dummyFC.state_dict()\n",
    "dummyFC_SD['weight'].copy_(checkpoint['state_dict']['dummyFC.weight'])\n",
    "dummyFC_SD['bias'].copy_(checkpoint['state_dict']['dummyFC.bias'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.0103, -0.0271, -0.0646, -0.0680, -0.0014, -0.0277])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "split = get_splits('cifar10', 0)\n",
    "known_data = SplitCifar10('/datasets', train=False, transform=val_transform, split=split['known_classes'])\n",
    "open_data = SplitCifar10('/datasets', train=False, transform=val_transform, split=split['unknown_classes']) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "known_loader = DataLoader(known_data, batch_size=128, shuffle=False, num_workers=4)\n",
    "open_loader = DataLoader(open_data, batch_size=128, shuffle=False, num_workers=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "tInput, tTarget = next(iter(known_loader))\n",
    "model.cuda()\n",
    "dummyFC.cuda()\n",
    "tInput, tTarget = tInput.cuda(), tTarget.cuda()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "logit, features = model(tInput, tTarget, return_features=[2,])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "dummy_output = dummyFC(features)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "logit.mean(0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([13.0603, 13.2038, 13.8284, 13.4362, 14.1509, 13.9361], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "dummy_output.mean(0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.1048,  0.2672,  0.1681, -0.1234,  0.4077, -0.0413], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "oInput, oTarget = next(iter(open_loader))\n",
    "oInput, oTarget = oInput.cuda(), oTarget.cuda()\n",
    "ologit, ofeatures = model(oInput, oTarget, return_features=[2,])\n",
    "oDummy_output = dummyFC(ofeatures)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "oDummy_output.mean(0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.1784,  0.2836,  0.1552, -0.1031,  0.4289, -0.0826], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "ologit.mean(0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([13.3796, 13.6405, 14.2513, 13.9078, 14.6790, 14.2687], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "a = torch.norm(ologit, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "b = torch.norm(oDummy_output, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "mrl = nn.MarginRankingLoss(margin=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "mrl(oDummy_output, ologit, torch.ones(1).cuda())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(18.9372, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}